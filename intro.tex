\section{Introduction}

Modern distributed data stores offer a choice of consistency
models. Weak consistency models are fast and guarantee ``always-on''
behavior, but provide limited guarantees. Stronger consistency models
are easier to reason about but are slower and potentially
unavailable. The choice of a consistency model has wide-ranging implications
for application writers, operations management, and
end-users. In light of its performance benefits, weak
consistency is often considered acceptable.

One commonly deployed weak consistency model, eventual
consistency, is, on its face, a cavalier proposition. Eventual
consistency provides no guarantees as to when new writes will become
visible to readers and what versions of data items will be presented
in the interim. For example, reading all \texttt{null} values from a
database satisfies eventual consistency. Similarly, the data store can
delay writes for weeks and not violate eventual
consistency guarantees. Yet, despite these weak semantics, there is
common sentiment among practitioners that eventual consistency is
often ``good enough'' and ``worthwhile'' for many
internet applications.

In recent work, we provided an analytical and empirical framework for
analyzing the consistency provided by eventually consistent stores,
called Probabilistically Bounded Staleness, or
PBS~\cite{pbs-vldb2012}. Eventually consistent stores do not make promises
about answers to the length of time required to observe an update or
the staleness of values, but this does not preclude us from making
informed statements about what is likely to happen. By using expert
knowledge of the underlying data store and its replication protocols
in addition to some lightweight in-situ profiling, we can inform data
store users about what consistency they are likely to observe in the
future. Thus, PBS provides a probabilistic answer to the question 
of ``how eventual and how consistent is eventual consistency?''

Our PBS predictions are part of a larger trend towards providing
quantitative measurements and analysis of weakly consistent
stores. There is recent work ranging from distributed systems theory
to database practitioner communities on measuring violations of
properties such as atomicity, serializability, and regular register
semantics~\cite{hotdep}. PBS in particular has experienced relative
popularity in the NoSQL community and our implementation of PBS for
quorum-based systems was recently accepted to Cassandra's mainline
source trunk and is slated for release by the end of
2012~\cite{cassandra-pbs-patch}. Our discussions with other
researchers indicate more technology transfers of monitoring
and verification tools in the future.

While monitoring and prediction are useful in themselves, perhaps more
importantly, they enable a rich space of applications that we believe
have been neglected. For example, in our initial PBS work, we mostly
assume that simply providing the predictions is a worthwhile task:
understanding their utility is left to future work. Among the many 
applications, we consider three of them to be most promising: \textit{i.)} dynamic
request-based consistency parameters, or auto-tuning request routing
based on a given latency and consistency service level agreement,
\textit{ii.)} database administration tasks with respect to the impact
of slow nodes and networks, replication factors, and data store
paramers like anti-entropy rates, and \textit{iii.)} integration into
traditional alerts and monitoring frameworks. We elaborate further in
Section~\ref{sec:scenarios}, but, in short, these quantitative metrics
allow new ways to tune the performance, semantics, and the maintenance
of eventually consistent stores.

In this demo proposal, we outline these advanced use cases in detail
(Section~\ref{sec:scenarios}), describe how both quantitative metrics
can be integrated into existing stores and architectures based on our
experiences with the Cassandra community
(Section~\ref{sec:architecture}), and sketch how we plan to
demonstrate this newly enabled functionality, allowing SIGMOD
attendees to act as end-users, operations managers, and
adversaries of an eventually consistent internet service
(Section~\ref{sec:demo}).

\subsection{PBS Metrics}
To provide background on consistency metrics, we briefly summarize the
metrics proposed by PBS~\cite{pbs-vldb2012}. Data
staleness can be expressed in terms of two metrics: wall-clock time
and versions.

\textbf{t-visibility:} $t$-visibility captures the ``window of
inconsistency`` in terms of wall clock time after a write happens. An
eventually consistent system will eventually respond to all read
requests with the last written value, and $t$-visibilty provides an expected
value for this ``eventuality.''  More formally, $t$-visibility is the
probability that a read operation starting $t$ seconds after the
previous write completed will see the most recent value from the data
store. For example, if a data store configuration has $t$-visibilty of
95\% at 100ms, it means that 100ms after the last write completes,
95\% of read operations will see the most recent value.

\textbf{k-staleness:} $k$-staleness determines the probability of a
read returning a value within a bounded number of versions. This
estimate can be useful for estimating the probability of getting 
monotonic reads, which promise that a user will
never read older versions than she has already read (i.e., does
not ``go back in time'').

\textbf{$\langle$k, t$\rangle$-staleness:} We can combine $t$-visibility
and $k$-staleness to consider both time- and version-based staleness
together, called $\langle k, t \rangle$-staleness. For example, if a
data store configuration has $\langle k, t \rangle$-staleness of 75\%
at 10ms and 3 versions, then 10ms after the last write completes, 75\%
of reads will return a value that is no more than 3 versions old.



