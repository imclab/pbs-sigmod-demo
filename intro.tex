\section{Introduction}

Modern distributed data stores offer a choice of consistency
models. Weak consistency models are fast and guarantee ``always-on''
behavior, but provide limited guarantees. Stronger consistency models
are easier to reason about but are slower and potentially
unavailable~\cite{davidson-survey}. The choice of a consistency model has
wide-ranging implications for application writers, operations
management, and end-users.  In light of its performance
benefits~\cite{abadi-pacelc}, weak consistency is often considered
acceptable.

One commonly deployed weak consistency model, eventual consistency,
is, on its face, a cavalier proposition. Eventual consistency provides
no guarantees as to when new writes will become visible to readers and
what versions of data items will be presented in the interim. For
example, reading all \texttt{null} values from a database satisfies
eventual consistency. Similarly, the data store can delay writes for
weeks and not violate eventual consistency
guarantees~\cite{vogels-defs}. Yet, despite these weak semantics,
there is common sentiment among practitioners that eventual
consistency is often ``good enough'' and ``worthwhile'' for many
internet applications.

In recent work, we provided an analytical and empirical framework for
analyzing the consistency provided by eventually consistent stores,
called Probabilistically Bounded Staleness, or
PBS~\cite{pbs-vldb2012}. Eventually consistent stores do not make
promises about the length of time required to observe an update or the
staleness of values, but this does not preclude us from making
informed statements about a store's expected behavior. By using expert
knowledge of the underlying data store and its replication protocols
in addition to performing lightweight in-situ profiling, we can inform
data store users about what consistency they are likely to observe in
the future. Thus, PBS provides a probabilistic answer to the question
of ``how eventual and how consistent is eventual consistency?''

PBS predictions are part of a larger trend towards providing
quantitative measurements and analysis of weakly consistent
stores. Recent work ranges from theoretical studies~\cite{podc-hpl} to
practitioner reports~\cite{hotdep} on measuring violations of
properties such as atomicity, serializability, and regular register
semantics. PBS in particular has experienced relative popularity in
the NoSQL community and our implementation of PBS for quorum-based
systems was integrated with Cassandra's mainline source and released
as a feature of the 1.2.0 release in January
2013~\cite{cassandra-pbs-patch}. Our discussions with other
researchers indicate the possibility of additional technology
transfer of monitoring and verification techniques in the future.

While monitoring and prediction are useful in themselves, perhaps more
importantly, they enable a rich space of applications that we believe
have been neglected. For example, in our initial PBS work, we assume
that providing predictions is a worthwhile task: understanding their
utility is left to future work. Yet there \textit{are} many useful
applications, among which we consider three to be the most promising:
\textit{i.)} dynamic request-based consistency parameters, or
auto-tuning request routing based on a latency- and consistency-based
service level agreement, \textit{ii.)} database administration tasks
with respect to the impact of slow nodes and networks, replication
factors, and data store paramers like anti-entropy rates, and
\textit{iii.)} integration into traditional alerting and monitoring
frameworks. We elaborate further in Section~\ref{sec:scenarios}, but,
in short, these quantitative metrics allow new ways to improve the
performance, semantics, and maintenance of eventually consistent
stores.

In this demo proposal, we outline these advanced use cases in detail
(Section~\ref{sec:scenarios}), describe how quantitative metrics
can be integrated into existing data stores based on our
experiences with the Cassandra community
(Section~\ref{sec:architecture}), and sketch how we plan to
demonstrate this newly enabled functionality, allowing SIGMOD
attendees to act as end-users, operations managers, and
adversaries of an eventually consistent internet service
(Section~\ref{sec:demo}).

\subsection{PBS Metrics}
To provide background on consistency metrics, we briefly summarize the
metrics proposed by PBS~\cite{pbs-vldb2012}. Data
staleness can be expressed in terms of two metrics: wall-clock time
and versions.

\textbf{t-visibility:} $t$-visibility captures the ``window of
inconsistency`` in terms of wall clock time after a write happens. An
eventually consistent system will eventually respond to all read
requests with the last written value, and $t$-visibility provides an expected
value for this ``eventuality.''  More formally, $t$-visibility is the
probability that a read operation starting $t$ seconds after the
previous write completed will see the most recent value from the data
store. For example, if a data store configuration has $t$-visibilty of
95\% at 100ms, it means that 100ms after the last write completes,
95\% of read operations will see the most recent value.

\textbf{k-staleness:} $k$-staleness determines the probability of a
read returning a value within a bounded number of versions. This
estimate can be useful for estimating the probability of experiencing
monotonic reads, under which users will never read older versions than
they have already read (i.e., reads not ``go back in
time'')~\cite{vogels-defs}.

\textbf{$\langle$k, t$\rangle$-staleness:} We can combine $t$-visibility
and $k$-staleness to consider both time- and version-based staleness
together, called $\langle k, t \rangle$-staleness. For example, if a
data store configuration has $\langle k, t \rangle$-staleness of 75\%
at 10ms and 3 versions, then 10ms after the last write completes, 75\%
of reads will return a value that is no more than 3 versions old.



