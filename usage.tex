\section{Usage Scenarios}
\label{sec:scenarios}

Quantative consistency metrics are useful for a variety of services
that can tolerate eventual consistency. In this section, we outline
three use cases in the context of a hypothetical microblogging web
service, Twissistency.

\subsection{Dynamic Reconfiguration}

\textit{Twissistency's data scientists have learned that their users
  respond negatively to high latencies, and the company sets a desired
  latency bound of 50ms for their back-end data store requests.}\\

Data stores often offer a choice between replication parameters. For
example, in Dynamo-style sytems, applications can choose the minimum
number of replicas to read from ($R$) and write to ($W$). If $R$+$W$
is greater than the number of replicas ($N$), operations will achieve
\textit{strong consistency}, and, if not, the system provides 
\textit{eventual consistency}. With a replication factor of $N=3$, we 
have three options for eventually consistent operation: 
$R$$=$$W$$=$$1$, $R$$=$$1$$, W$$=$$2$, and $R$$=$$2$$, W$$=$$1$. 
$R$$=$$W$$=$$1$ is guaranteed to be fastest, but it is also the least 
consistent.

\textit{Based on feedback from their infrastructure team, the
  application developers set $R$=$W$=$1$ for their Dynamo-based data
  store. This meets the latency SLA with a 99th percentile latency of 30ms,
  but they notice that many requests return stale data. They would
  like to use $R$=$W$=$2$, but this is too slow. Further they notice that the
  number of stale reads with $R$=$W$=$1$ increases when there are hotspots 
  or failures in their cluster.}\\

It is non-trivial for users to choose what is the right value for $R$ and $W$
for their cluster configuration and hence users are not able to effectively use
the flexibility provided by their data stores. Further, it would be easier for
application programmers to state their higher-level SLAs in terms of the the 
99th percentile latency or \textit{t-visibility} and let the system adjust its 
parameters to meet this goal. Hence we believe that automatic and dynamic 
reconfiguration of parameters is an important use case for many deployments. 

\subsection{DBA Integration}

\textit{The datastore administrators at Twissistency have received reports that
  some users are seeing very inconsistent data from their distributed datastore. 
  They wish to study what could be the causes for this drop in consistency and
  how would changing system parameters like anti-entropy rate affect this.
  However they would like to understand the implications of making these changes 
  before modifying their production datastore.}\\

There are a number of system parameters which affect the performance and
observed consistency of a distributed datastore. Datastore administrators are
often limited by two major shortcomings: a. limited information available in terms
of the currently observed consistency properties and b. lack of easy methods to
understand how the system will behave as parameters are varied.

The cause for inconsistent reads could stem from the fact that there are slow
nodes in the cluster or that there are some keys which are hotspots. The
possible set of actions that a database administrator could choose include
changing the number of replicas for a particular key to modifying the rate of
anti-entropy operations like ReadRepair. We believe that providing better
metrics and models for consistency will help database administrators rapidly 
identify problems and measure the effects of proposed changes.

% Replication parameter
% Read repair rate
% Slow nodes

\subsection{Monitoring and Alerts}

\textit{Twissistency has a large number of DevOps who are responsible for
  keeping their service up and running. Currently their monitoring and pager
  service is triggered when user-visible latency is high or if some of their
  tasks are not running. As user experience is negatively impacted by
  inconsistent reads, the CTO wants to ensure that DevOps respond to these
  alerts too.}\\

%Devops get paged in the middle of the night

Devops monitoring systems are typically driven by rule-based triggers on
timeseries data. This is suitable for alerts on latency, throughput and failures
but not readily amenable to monitoring the user-visible consistency. In order to
do so, we will need to export consistency using metrics that can be used for
alerting and also provide tools that can be used to respond to such alerts. 
